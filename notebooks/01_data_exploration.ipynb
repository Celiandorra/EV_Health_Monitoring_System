{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bafddfcb",
      "metadata": {},
      "source": [
        "# Advanced EV Health Monitoring and Predictive Maintenance\n",
        "## 01 - Dataset Loading and Initial Exploration\n",
        "\n",
        "This notebook explores the two main datasets:\n",
        "1. **EVIoT-PredictiveMaint Dataset** (15-minute intervals) - Archive folder\n",
        "2. **EV Sensors: Driving Pattern Diagnostics** (2020-24) - Archive(1) folder\n",
        "\n",
        "The goal is to understand the data structure, quality, and potential for integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f9dee3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c1d9e5",
      "metadata": {},
      "source": [
        "## 1. Load EVIoT-PredictiveMaint Dataset (15-min intervals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb436310",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the predictive maintenance dataset\n",
        "maintenance_data_path = '../archive/EV_Predictive_Maintenance_Dataset_15min.csv'\n",
        "\n",
        "try:\n",
        "    maintenance_df = pd.read_csv(maintenance_data_path)\n",
        "    print(f\"\u2705 Loaded maintenance dataset: {maintenance_df.shape}\")\n",
        "    print(f\"\ud83d\udcca Columns: {list(maintenance_df.columns)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\u274c File not found. Please check the path.\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error loading file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc08aea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic information about the maintenance dataset\n",
        "print(\"=== MAINTENANCE DATASET INFO ===\")\n",
        "print(f\"Shape: {maintenance_df.shape}\")\n",
        "print(f\"\\nData types:\")\n",
        "print(maintenance_df.dtypes)\n",
        "print(f\"\\nMemory usage: {maintenance_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511ae10d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the first few rows\n",
        "print(\"=== FIRST 5 ROWS ===\")\n",
        "display(maintenance_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd151527",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
        "missing_values = maintenance_df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(maintenance_df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_values,\n",
        "    'Missing Percentage': missing_percentage\n",
        "}).sort_values('Missing Percentage', ascending=False)\n",
        "\n",
        "# Display only columns with missing values\n",
        "missing_df_filtered = missing_df[missing_df['Missing Count'] > 0]\n",
        "if not missing_df_filtered.empty:\n",
        "    display(missing_df_filtered)\n",
        "else:\n",
        "    print(\"\u2705 No missing values found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7796f0",
      "metadata": {},
      "source": [
        "## 2. Load EV Sensors: Driving Pattern Diagnostics (2020-24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d780369d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all driving pattern datasets\n",
        "import os\n",
        "\n",
        "pattern_data_path = '../archive (1)/'\n",
        "pattern_files = {\n",
        "    'rare_user': 'rare_user.csv',\n",
        "    'moderate_user': 'moderate_user.csv',\n",
        "    'heavy_user': 'heavy_user.csv',\n",
        "    'daily_user': 'daily_user.csv'\n",
        "}\n",
        "\n",
        "pattern_dfs = {}\n",
        "\n",
        "for user_type, filename in pattern_files.items():\n",
        "    file_path = os.path.join(pattern_data_path, filename)\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['user_type'] = user_type  # Add user type identifier\n",
        "        pattern_dfs[user_type] = df\n",
        "        print(f\"\u2705 Loaded {user_type}: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\u274c File not found: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error loading {filename}: {e}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Total pattern datasets loaded: {len(pattern_dfs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cacc616",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all pattern datasets into one dataframe\n",
        "if pattern_dfs:\n",
        "    combined_patterns_df = pd.concat(pattern_dfs.values(), ignore_index=True)\n",
        "    print(f\"\u2705 Combined patterns dataset: {combined_patterns_df.shape}\")\n",
        "    print(f\"\ud83d\udcca Columns: {list(combined_patterns_df.columns)}\")\n",
        "    \n",
        "    # Check distribution of user types\n",
        "    print(f\"\\n\ud83d\udcc8 User type distribution:\")\n",
        "    print(combined_patterns_df['user_type'].value_counts())\n",
        "else:\n",
        "    print(\"\u274c No pattern datasets loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b3cbef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample from patterns dataset\n",
        "print(\"=== PATTERNS DATASET SAMPLE ===\")\n",
        "display(combined_patterns_df.head())\n",
        "\n",
        "print(f\"\\n=== DATA TYPES ===\")\n",
        "print(combined_patterns_df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427fa2ae",
      "metadata": {},
      "source": [
        "## 3. Data Quality Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253d683b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary for maintenance dataset\n",
        "print(\"=== MAINTENANCE DATASET STATISTICS ===\")\n",
        "display(maintenance_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc1dd8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary for patterns dataset\n",
        "print(\"=== PATTERNS DATASET STATISTICS ===\")\n",
        "# Select only numeric columns for description\n",
        "numeric_cols = combined_patterns_df.select_dtypes(include=[np.number]).columns\n",
        "display(combined_patterns_df[numeric_cols].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d91ca0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "print(\"=== DUPLICATE ANALYSIS ===\")\n",
        "maintenance_duplicates = maintenance_df.duplicated().sum()\n",
        "patterns_duplicates = combined_patterns_df.duplicated().sum()\n",
        "\n",
        "print(f\"Maintenance dataset duplicates: {maintenance_duplicates}\")\n",
        "print(f\"Patterns dataset duplicates: {patterns_duplicates}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012823b8",
      "metadata": {},
      "source": [
        "## 4. Initial Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ffe839",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize key sensor distributions for maintenance dataset\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Key Sensor Distributions - Maintenance Dataset', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Select key sensors to visualize\n",
        "key_sensors = ['SoC', 'SoH', 'Battery_Temperature', 'Motor_Temperature', 'RUL', 'Failure_Probability']\n",
        "\n",
        "for i, sensor in enumerate(key_sensors):\n",
        "    if sensor in maintenance_df.columns:\n",
        "        row = i // 3\n",
        "        col = i % 3\n",
        "        axes[row, col].hist(maintenance_df[sensor].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
        "        axes[row, col].set_title(f'{sensor} Distribution')\n",
        "        axes[row, col].set_xlabel(sensor)\n",
        "        axes[row, col].set_ylabel('Frequency')\n",
        "        axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3619511a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize SOC and SOH patterns across different user types\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('SOC and SOH Patterns by User Type', fontsize=16, fontweight='bold')\n",
        "\n",
        "user_types = combined_patterns_df['user_type'].unique()\n",
        "\n",
        "for i, user_type in enumerate(user_types):\n",
        "    user_data = combined_patterns_df[combined_patterns_df['user_type'] == user_type]\n",
        "    \n",
        "    row = i // 2\n",
        "    col = i % 2\n",
        "    \n",
        "    # Plot SOC vs SOH\n",
        "    scatter = axes[row, col].scatter(user_data['SOC'], user_data['SOH'], \n",
        "                                   alpha=0.6, s=1)\n",
        "    axes[row, col].set_title(f'{user_type.replace(\"_\", \" \").title()} - SOC vs SOH')\n",
        "    axes[row, col].set_xlabel('State of Charge (%)')\n",
        "    axes[row, col].set_ylabel('State of Health (%)')\n",
        "    axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc04fa31",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive visualization using Plotly\n",
        "# SOC distribution by user type\n",
        "fig = px.box(combined_patterns_df, x='user_type', y='SOC', \n",
        "             title='State of Charge Distribution by User Type',\n",
        "             labels={'user_type': 'User Type', 'SOC': 'State of Charge (%)'})\n",
        "\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79dabee5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Battery temperature patterns\n",
        "fig = px.violin(combined_patterns_df, x='user_type', y='Battery_Temp',\n",
        "                title='Battery Temperature Distribution by User Type',\n",
        "                labels={'user_type': 'User Type', 'Battery_Temp': 'Battery Temperature (\u00b0C)'})\n",
        "\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746b1b7f",
      "metadata": {},
      "source": [
        "## 5. Common Features Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b105b52f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify common features between datasets\n",
        "maintenance_cols = set(maintenance_df.columns)\n",
        "patterns_cols = set(combined_patterns_df.columns)\n",
        "\n",
        "common_features = maintenance_cols.intersection(patterns_cols)\n",
        "maintenance_only = maintenance_cols - patterns_cols\n",
        "patterns_only = patterns_cols - maintenance_cols\n",
        "\n",
        "print(\"=== FEATURE COMPARISON ===\")\n",
        "print(f\"Common features ({len(common_features)}): {sorted(common_features)}\")\n",
        "print(f\"\\nMaintenance dataset only ({len(maintenance_only)}): {sorted(maintenance_only)}\")\n",
        "print(f\"\\nPatterns dataset only ({len(patterns_only)}): {sorted(patterns_only)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44fd1674",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis for common numeric features\n",
        "if common_features:\n",
        "    # Get numeric common features\n",
        "    numeric_common = []\n",
        "    for feature in common_features:\n",
        "        if (pd.api.types.is_numeric_dtype(maintenance_df[feature]) and \n",
        "            pd.api.types.is_numeric_dtype(combined_patterns_df[feature])):\n",
        "            numeric_common.append(feature)\n",
        "    \n",
        "    if numeric_common:\n",
        "        print(f\"Numeric common features: {numeric_common}\")\n",
        "        \n",
        "        # Create correlation matrix for maintenance dataset\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        corr_maintenance = maintenance_df[numeric_common].corr()\n",
        "        sns.heatmap(corr_maintenance, annot=True, cmap='coolwarm', center=0,\n",
        "                    square=True, fmt='.2f')\n",
        "        plt.title('Correlation Matrix - Maintenance Dataset (Common Features)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Create correlation matrix for patterns dataset\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        corr_patterns = combined_patterns_df[numeric_common].corr()\n",
        "        sns.heatmap(corr_patterns, annot=True, cmap='coolwarm', center=0,\n",
        "                    square=True, fmt='.2f')\n",
        "        plt.title('Correlation Matrix - Patterns Dataset (Common Features)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No numeric common features found for correlation analysis.\")\n",
        "else:\n",
        "    print(\"No common features found between datasets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f058f0a2",
      "metadata": {},
      "source": [
        "## 6. Temporal Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb7b1ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse timestamps for temporal analysis\n",
        "# For maintenance dataset\n",
        "if 'Timestamp' in maintenance_df.columns:\n",
        "    maintenance_df['Timestamp'] = pd.to_datetime(maintenance_df['Timestamp'])\n",
        "    print(f\"Maintenance dataset time range: {maintenance_df['Timestamp'].min()} to {maintenance_df['Timestamp'].max()}\")\n",
        "    print(f\"Maintenance dataset frequency: {(maintenance_df['Timestamp'].iloc[1] - maintenance_df['Timestamp'].iloc[0])}\")\n",
        "\n",
        "# For patterns dataset (assuming first column is timestamp)\n",
        "timestamp_col = combined_patterns_df.columns[0]\n",
        "if timestamp_col:\n",
        "    combined_patterns_df[timestamp_col] = pd.to_datetime(combined_patterns_df[timestamp_col])\n",
        "    print(f\"\\nPatterns dataset time range: {combined_patterns_df[timestamp_col].min()} to {combined_patterns_df[timestamp_col].max()}\")\n",
        "    \n",
        "    # Check frequency for each user type\n",
        "    for user_type in combined_patterns_df['user_type'].unique():\n",
        "        user_data = combined_patterns_df[combined_patterns_df['user_type'] == user_type].sort_values(timestamp_col)\n",
        "        if len(user_data) > 1:\n",
        "            freq = user_data[timestamp_col].iloc[1] - user_data[timestamp_col].iloc[0]\n",
        "            print(f\"{user_type} frequency: {freq}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd29d275",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series plot for SOC patterns\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
        "fig.suptitle('SOC Time Series by User Type (First 1000 records)', fontsize=16, fontweight='bold')\n",
        "\n",
        "user_types = combined_patterns_df['user_type'].unique()\n",
        "\n",
        "for i, user_type in enumerate(user_types):\n",
        "    user_data = combined_patterns_df[combined_patterns_df['user_type'] == user_type].head(1000)\n",
        "    \n",
        "    row = i // 2\n",
        "    col = i % 2\n",
        "    \n",
        "    axes[row, col].plot(user_data[timestamp_col], user_data['SOC'], alpha=0.7, linewidth=0.5)\n",
        "    axes[row, col].set_title(f'{user_type.replace(\"_\", \" \").title()} - SOC Over Time')\n",
        "    axes[row, col].set_xlabel('Time')\n",
        "    axes[row, col].set_ylabel('State of Charge (%)')\n",
        "    axes[row, col].grid(True, alpha=0.3)\n",
        "    axes[row, col].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2290e4c8",
      "metadata": {},
      "source": [
        "## 7. Key Insights Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf59005",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== DATA EXPLORATION SUMMARY ===\")\n",
        "print(\"\\n\ud83d\udcca Dataset Overview:\")\n",
        "print(f\"\u2022 Maintenance dataset: {maintenance_df.shape[0]:,} records, {maintenance_df.shape[1]} features\")\n",
        "print(f\"\u2022 Patterns dataset: {combined_patterns_df.shape[0]:,} records, {combined_patterns_df.shape[1]} features\")\n",
        "print(f\"\u2022 Total combined records: {maintenance_df.shape[0] + combined_patterns_df.shape[0]:,}\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Key Targets:\")\n",
        "if 'RUL' in maintenance_df.columns:\n",
        "    print(f\"\u2022 RUL (Remaining Useful Life): {maintenance_df['RUL'].min():.1f} - {maintenance_df['RUL'].max():.1f}\")\n",
        "if 'Failure_Probability' in maintenance_df.columns:\n",
        "    print(f\"\u2022 Failure Probability: {maintenance_df['Failure_Probability'].min():.3f} - {maintenance_df['Failure_Probability'].max():.3f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udc65 User Profiles:\")\n",
        "for user_type in combined_patterns_df['user_type'].unique():\n",
        "    count = len(combined_patterns_df[combined_patterns_df['user_type'] == user_type])\n",
        "    print(f\"\u2022 {user_type.replace('_', ' ').title()}: {count:,} records\")\n",
        "\n",
        "print(\"\\n\ud83d\udd17 Integration Potential:\")\n",
        "print(f\"\u2022 Common features: {len(common_features)} features can be directly compared\")\n",
        "print(f\"\u2022 Temporal alignment: Both datasets span multiple years with regular intervals\")\n",
        "print(f\"\u2022 Feature complementarity: {len(maintenance_only)} unique maintenance features + {len(patterns_only)} unique pattern features\")\n",
        "\n",
        "print(\"\\n\u2705 Next Steps:\")\n",
        "print(\"1. Data preprocessing and cleaning\")\n",
        "print(\"2. Feature engineering and temporal alignment\")\n",
        "print(\"3. Dataset integration and harmonization\")\n",
        "print(\"4. Predictive model development\")\n",
        "print(\"5. Personalized recommendation system\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1d5d078",
      "metadata": {},
      "source": [
        "## 8. Save Processed Data for Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7464824",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary report\n",
        "summary_report = {\n",
        "    'datasets': {\n",
        "        'maintenance': {\n",
        "            'shape': maintenance_df.shape,\n",
        "            'columns': list(maintenance_df.columns),\n",
        "            'memory_mb': maintenance_df.memory_usage(deep=True).sum() / 1024**2\n",
        "        },\n",
        "        'patterns': {\n",
        "            'shape': combined_patterns_df.shape,\n",
        "            'columns': list(combined_patterns_df.columns),\n",
        "            'user_types': list(combined_patterns_df['user_type'].unique()),\n",
        "            'memory_mb': combined_patterns_df.memory_usage(deep=True).sum() / 1024**2\n",
        "        }\n",
        "    },\n",
        "    'integration': {\n",
        "        'common_features': list(common_features),\n",
        "        'maintenance_only': list(maintenance_only),\n",
        "        'patterns_only': list(patterns_only)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save summary for reference\n",
        "import json\n",
        "with open('../reports/data_exploration_summary.json', 'w') as f:\n",
        "    json.dump(summary_report, f, indent=2, default=str)\n",
        "\n",
        "print(\"\u2705 Data exploration complete!\")\n",
        "print(\"\ud83d\udcc4 Summary saved to: ../reports/data_exploration_summary.json\")\n",
        "print(\"\ud83d\udccb Ready for Phase 2: Data Integration & Preprocessing\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}