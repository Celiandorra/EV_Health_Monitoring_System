{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "65c77c78",
      "metadata": {},
      "source": [
        "# LSTM Model for RUL Prediction\n",
        "\n",
        "This notebook builds and trains an LSTM (Long Short-Term Memory) model to predict the `overall_health_score` as a proxy for RUL. LSTMs are well-suited for time-series data as they can capture temporal dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f03fa96f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9b519e",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n",
        "\n",
        "We'll load the engineered features dataset. For the LSTM, we need to scale the features and then reshape the data into sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bb6a098",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('../data/features/features_for_modeling.csv')\n",
        "\n",
        "# --- FIX: Impute NaN values with 0 ---\n",
        "# These NaNs likely occur for users without a historical average.\n",
        "# Imputing with 0 assumes no deviation from the average.\n",
        "impute_cols = [\n",
        "    'Ambient_Humidity_vs_user_avg',\n",
        "    'Ambient_Temperature_vs_user_avg',\n",
        "    'Battery_Current_vs_user_avg',\n",
        "    'Battery_Voltage_vs_user_avg'\n",
        "]\n",
        "for col in impute_cols:\n",
        "    if col in data.columns:\n",
        "        data[col].fillna(0, inplace=True)\n",
        "\n",
        "# Also, drop any remaining NaN values from other columns just in case\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Define the target variable\n",
        "target_variable = 'overall_health_score'\n",
        "\n",
        "# Ensure all feature columns are numeric before scaling\n",
        "features = data.drop(columns=[target_variable])\n",
        "features = features.select_dtypes(include=np.number)\n",
        "target = data[target_variable]\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "print(f\"Features shape: {features.shape}\")\n",
        "print(f\"Target shape: {target.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b5fe76d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values in the dataframe\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values[missing_values > 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e8af0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features to a range between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Function to create sequences\n",
        "def create_sequences(features, target, time_steps=24):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - time_steps):\n",
        "        X.append(features[i:(i + time_steps)])\n",
        "        y.append(target[i + time_steps])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create sequences\n",
        "X_seq, y_seq = create_sequences(scaled_features, target.values, time_steps=24)\n",
        "\n",
        "print(f\"Shape of sequence data (X): {X_seq.shape}\")\n",
        "print(f\"Shape of sequence labels (y): {y_seq.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338c6477",
      "metadata": {},
      "source": [
        "## 2. Split Data into Training and Testing Sets\n",
        "\n",
        "We'll split the sequenced data into training and testing sets to evaluate the model's performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df37023",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e374a637",
      "metadata": {},
      "source": [
        "## 3. Build the LSTM Model\n",
        "\n",
        "We will construct a Sequential LSTM model with the following layers:\n",
        "- **LSTM Layer**: The core layer with 50 units to capture temporal patterns. `return_sequences=True` is used for stacking LSTM layers.\n",
        "- **Dropout Layer**: To prevent overfitting by randomly setting a fraction of input units to 0.\n",
        "- **Dense Layer**: A fully connected layer to produce the final prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f933a74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# --- FIX: Use an optimizer with gradient clipping to prevent NaN loss ---\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21d3c97d",
      "metadata": {},
      "source": [
        "## 4. Train the LSTM Model\n",
        "\n",
        "Now, we'll train the model on the training data. We'll use a validation split to monitor performance on a subset of the training data and use `EarlyStopping` to prevent overfitting by stopping the training if the validation loss doesn't improve for 10 consecutive epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "773c8e09",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adcdc115",
      "metadata": {},
      "source": [
        "## 5. Evaluate Model Performance\n",
        "\n",
        "After training, we'll evaluate the model's performance on the test set using RMSE and MAE. This will tell us how well the model generalizes to new, unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6937039",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE and MAE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"LSTM Model Test RMSE: {rmse:.4f}\")\n",
        "print(f\"LSTM Model Test MAE: {mae:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c8b8cb9",
      "metadata": {},
      "source": [
        "## 6. Visualize Training History\n",
        "\n",
        "Plotting the training and validation loss over epochs helps us understand how the model learned and whether it overfitted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d85832d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTM Model Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e096880",
      "metadata": {},
      "source": [
        "## 7. Visualize Predictions vs. Actual Values\n",
        "\n",
        "A scatter plot of predicted vs. actual values can give us a good visual sense of the model's performance. A perfect model would have all points on the diagonal line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afeb9f07",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(y_test, y_pred, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)\n",
        "plt.title('Actual vs. Predicted Health Score')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e68aec7",
      "metadata": {},
      "source": [
        "## 8. Hyperparameter Tuning: A More Complex LSTM\n",
        "\n",
        "The initial LSTM model's performance was not as good as the tree-based models. Let's try a more complex architecture to see if we can capture more intricate patterns. We will increase the number of units in the LSTM layers and add an extra Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d7828a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a more complex LSTM model\n",
        "tuned_model = Sequential([\n",
        "    LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dropout(0.3),\n",
        "    LSTM(100, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with gradient clipping\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n",
        "tuned_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "tuned_model.summary()\n",
        "\n",
        "# Train the new model\n",
        "history_tuned = tuned_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the tuned model\n",
        "y_pred_tuned = tuned_model.predict(X_test)\n",
        "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
        "mae_tuned = mean_absolute_error(y_test, y_pred_tuned)\n",
        "\n",
        "print(f\"Tuned LSTM Model Test RMSE: {rmse_tuned:.4f}\")\n",
        "print(f\"Tuned LSTM Model Test MAE: {mae_tuned:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}